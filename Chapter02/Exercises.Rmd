---
title: "Chapter 2"
author: "Wooyoung Jang"
date: "6/16/2021"
output: html_document
---

<!-- # Chapter 2: Random Variable Generation -->

<!-- ## Exercise 2.1 -->
<!-- Note that $u \leq F(F^-(u))$ and $x \geq F^-(F(x))$. Show that $P(F^-(U) \leq x) \leq P(U \leq F(x)) \leq P(F^-(U) \leq x)$. The result directly follows. -->

<!-- ## Exercise 2.2 -->
<!-- ### (a) -->
<!-- ```{r} -->
<!-- supp = seq(-12, 12, 0.01) -->
<!-- N = 10000 -->
<!-- U = runif(N) -->
<!-- f = dlogis -->
<!-- Finv = function(x) { -log((1-x)/x) } -->
<!-- X = Finv(U); X = X[-12 < X & X < 12] -->
<!-- Y = rlogis(N); Y = Y[-12 < Y & Y < 12] -->

<!-- par(mfrow=c(1,2)) -->
<!-- hist(X, freq=FALSE, breaks=seq(-12, 12, 0.5), main="Logistic from Uniform") -->
<!-- lines(supp, f(supp), col="red", lwd=2) -->
<!-- hist(Y, freq=FALSE, breaks=seq(-12, 12, 0.5), main="Logistic from R") -->
<!-- lines(supp, f(supp), col="red", lwd=2) -->
<!-- ``` -->

<!-- ### (b) -->
<!-- ```{r} -->
<!-- supp = seq(-12, 12, 0.01) -->
<!-- N = 10000 -->
<!-- U = runif(N) -->
<!-- f = dcauchy -->
<!-- Finv = function(x) { tan(pi * (x - 1/2)) } -->
<!-- X = Finv(U); X = X[-12 < X & X < 12] -->
<!-- Y = rcauchy(N); Y = Y[-12 < Y & Y < 12] -->

<!-- par(mfrow=c(1,2)) -->
<!-- hist(X, freq=FALSE, breaks=seq(-12, 12, 0.5), main="Cauchy from Uniform") -->
<!-- lines(supp, f(supp), col="red", lwd=2) -->
<!-- hist(Y, freq=FALSE, breaks=seq(-12, 12, 0.5), main="Cauchy from R") -->
<!-- lines(supp, f(supp), col="red", lwd=2) -->
<!-- ``` -->

<!-- ## Exercise 2.3 -->
<!-- ### (a) -->
<!-- Note that $\mathbb{E}[U] =\frac{1}{2}$ and $Var(U) = \frac{1}{12}$ if $U \sim U(0, 1)$. -->

<!-- ### (b) -->
<!-- ```{r} -->
<!-- N = 10000 -->
<!-- test1 = function(N) { -->
<!--   U = matrix(runif(12 * N), nrow=12) - 0.5 -->
<!--   Z = apply(U, 2, sum) -->
<!-- } -->
<!-- test2 = function(N) { -->
<!--   U1 = runif(N) -->
<!--   U2 = runif(N) -->
<!--   X1 = sqrt(-2 * log(U1)) * cos(2 * pi * U2) -->
<!-- } -->
<!-- par(mfrow=c(1, 2)) -->
<!-- hist(test1(N), freq=FALSE, breaks=seq(-6, 6, 0.2), main="CLT-normal generator") -->
<!-- hist(test2(N), freq=FALSE, breaks=seq(-6, 6, 0.2), main="Box-Muller algorithm") -->

<!-- system.time(test1(N)); system.time(test2(N)) -->
<!-- ``` -->

<!-- ### (c) -->
<!-- ```{r} -->
<!-- test3 = function(N) { -->
<!--   rnorm(N) -->
<!-- } -->
<!-- hist(test3(N), freq=FALSE, breaks=seq(-6, 6, 0.2), main="Normal from R") -->
<!-- system.time(test3(N)) -->
<!-- ``` -->

<!-- ## Exercise 2.4 -->
<!-- ### (a), (b) -->
<!-- Omit. -->

<!-- ### (c) -->
<!-- ```{r} -->
<!-- N = 10000 -->
<!-- Sigma = cov(matrix(rnorm(30), nrow=10)) -->

<!-- test1 = function(N) { -->
<!--   A = t(chol(Sigma)) -->
<!--   X = matrix(runif(3 * N), nrow=N) -->
<!--   X = X %*% A -->
<!-- } -->
<!-- test2 = function(N) { -->
<!--   library(mnormt) -->
<!--   rmnorm(N, varcov=Sigma) -->
<!-- } -->

<!-- system.time(test1(N)); system.time(test2(N)) -->
<!-- ``` -->


<!-- ## Exercise 2.5 -->
<!-- - $P(U \leq f(Y)/Mg(Y)) = \int_{-\infty}^\infty \int_0^{f(y)/Mg(y)} du \cdot g(y) dy = \int_{-\infty}^\infty \frac{f(y)}{Mg(y)} \cdot g(y) dy = \frac{1}{M}.$ -->
<!-- - Let $\tilde{f} / \tilde{g} = cf/g$ for some $c$. Then we can observe that -->
<!-- $\mathbb{E}[1_{(U < \tilde{f}(Y) / \tilde{M} \tilde{g}(Y))}] = \int_{-\infty}^\infty \int_0^{\tilde{f}(y)/\tilde{M}\tilde{g}(y)} du \cdot g(y)dy = \int_{-\infty}^\infty cf(y)/\tilde{M}g(y) \cdot g(y) dy = c/\tilde{M}$. -->
<!-- Note that we can estimate $\mathbb{E}[1_{(U < \tilde{f}/\tilde{M}\tilde{g})}]$ by the empirical acceptance rate. Since we know $\tilde{M}$, we can estimate $c$ as well. -->


<!-- ## Exercise 2.6 -->
<!-- ## Exercise 2.7 -->


<!-- ## Exercise 2.8 -->
<!-- ### (a) -->

<!-- - $\frac{f(x)}{g(x|\alpha)} = \sqrt{\frac{2}{\pi}}\alpha^{-1} e^{-x^2/2 + \alpha|x|} \leq \sqrt{\frac{2}{\pi}} \alpha^{-1} e^{\alpha^2/2} \quad (\because \alpha > 0)$ -->
<!-- - Differentiating $\sqrt{\frac{2}{\pi}} \alpha^{-1} e^{\alpha^2/2}$ with respect to $\alpha$, we can find that the term is minimized when $\alpha = 1$. -->

<!-- ### (b) -->
<!-- Plug in $\alpha = 1$ and the result follows. The acceptance rate is given by $1/M$. -->

<!-- ### (c) -->
<!-- The inverse of cdf is give as $F^{-1}(y | \alpha) = \begin{cases} \frac{\log 2y}{\alpha} \quad & \text{if } 0 < y < \frac{1}{2} \\ -\frac{\log 2(1-y)}{\alpha} & \text{if } \frac{1}{2} \leq y < 1 \end{cases}$. We implement this function and then apply Accept-reject methods. -->

<!-- ```{r} -->
<!-- library(nimble) -->
<!-- N = 10000 -->

<!-- f_inv = function(x, a) { -->
<!--   f = function(x) { -->
<!--     if ((0 <= x) && (x < 1/2)) { -->
<!--       return(log(2*x)/a) -->
<!--     } -->
<!--     if ((1/2 <= x) && (x <= 1)) { -->
<!--       return(-log(2*(1-x))/a) -->
<!--     } -->
<!--   } -->
<!--   return(sapply(x, f)) -->
<!-- } -->

<!-- test1 = function(N, a) { -->
<!--   Y = f_inv(runif(N), a) -->
<!--   M = sqrt(2*exp(1) / pi) -->
<!--   U = runif(N) -->
<!--   X = Y[U*ddexp(Y) < dnorm(Y)/M] -->
<!--   return(X) -->
<!-- } -->

<!-- test2 = function(N) { -->
<!--   U1 = runif(N) -->
<!--   U2 = runif(N) -->
<!--   X = sqrt(-2*log(U1)) * cos(2*pi*U2) -->
<!--   return(X) -->
<!-- } -->

<!-- Xrange = seq(-4, 4, 0.001) -->
<!-- X1 = test1(N, 1) -->
<!-- X2 = test2(N) -->

<!-- par(mfrow=c(1, 2)) -->
<!-- hist(X1, freq=FALSE, main="Generated by Double Exponential") -->
<!-- lines(Xrange, dnorm(Xrange), col="sienna", lwd=2) -->
<!-- hist(X2, freq=FALSE, main="Generated by Box-Muller") -->
<!-- lines(Xrange, dnorm(Xrange), col="sienna", lwd=2) -->

<!-- system.time(test1(N, 1)) -->
<!-- system.time(test2(N)) -->
<!-- ``` -->




<!-- ## Exercise 2.9 -->
<!-- ## Exercise 2.10 -->

<!-- ## Exercise 2.11 -->
<!-- ### (a) -->
<!-- ```{R} -->
<!-- N = 10000 -->
<!-- n = 25 -->
<!-- p = 0.2 -->
<!-- test1 = function(N, n, p) { -->
<!--   prob = pbinom(0:n, n, p) -->
<!--   X = rep(0, N) -->
<!--   for (i in 1:N) { -->
<!--     u = runif(1) -->
<!--     X[i] = sum(prob < u) -->
<!--   } -->
<!--   return(X) -->
<!-- } -->
<!-- test2 = function(N, n, p) { -->
<!--   return(rbinom(N, n, p)) -->
<!-- } -->

<!-- system.time(test1(N, n, p)) -->
<!-- system.time(test2(N, n, p)) -->

<!-- X1 = test1(N, n, p) -->
<!-- X2 = test2(N, n, p) -->

<!-- par(mfrow=c(1, 2)) -->
<!-- hist(X1, freq=FALSE, main="Generaed by inverse transform") -->
<!-- lines(0:n, dbinom(0:n, n, p), col="sienna", lwd=2) -->
<!-- hist(X2, freq=FALSE, main="Generated by rbinom") -->
<!-- lines(0:n, dbinom(0:n, n, p), col="sienna", lwd=2) -->
<!-- ``` -->

<!-- ### (b) -->
<!-- Interpret the code as the Accept-Reject method. Then one can check that the code in the exercise generates a uniformly distributed random variable with maximum $\alpha$. -->
<!-- ```{R} -->
<!-- N = 10000 -->
<!-- a = 0.2 -->
<!-- test1 = function(N, a) { -->
<!--   X = rep(0, N) -->
<!--   for (i in 1:N) { -->
<!--     u = 2 -->
<!--     while(u > a) u = runif(1) -->
<!--     X[i] = u -->
<!--   } -->
<!--   return(X) -->
<!-- } -->
<!-- test2 = function(N, a) { -->
<!--   U = runif(N) -->
<!--   U = a * U -->
<!--   return(U) -->
<!-- } -->
<!-- test3 = function(N, a) { -->
<!--   return(runif(N, max=a)) -->
<!-- } -->

<!-- system.time(test1(N, a)) -->
<!-- system.time(test2(N, a)) -->
<!-- system.time(test3(N, a)) -->

<!-- X1 = test1(N, a) -->
<!-- X2 = test2(N, a) -->
<!-- X3 = test3(N, a) -->
<!-- Xrange = seq(0, a, 0.01) -->

<!-- par(mfrow=c(1, 3)) -->
<!-- hist(X1, freq=FALSE, main="Generated by the exercise code") -->
<!-- lines(Xrange, dunif(Xrange)/a, col="sienna", lwd=2) -->
<!-- hist(X2, freq=FALSE, main="Generated by aU") -->
<!-- lines(Xrange, dunif(Xrange)/a, col="sienna", lwd=2) -->
<!-- hist(X3, freq=FALSE, main="Generated by U(0, a)") -->
<!-- lines(Xrange, dunif(Xrange)/a, col="sienna", lwd=2) -->
<!-- ``` -->

<!-- ## Exercise 2.12 -->
<!-- ### (a) -->
<!-- ```{r} -->
<!-- a = 3 -->
<!-- b = 5 -->
<!-- N = 10^4 -->

<!-- test1 = function(N, a, b) { -->
<!--   U = matrix(runif(a * N), nrow=a) -->
<!--   X = b * apply(-log(U), 2, sum) -->
<!-- } -->
<!-- test2 = function(N, a, b) { -->
<!--   X = rgamma(N, a, scale=b) -->
<!-- } -->
<!-- system.time(test1(N, a, b)); system.time(test2(N, a, b)) -->

<!-- test1 = function(N, a, b) { -->
<!--   U = matrix(runif((a+b) * N), nrow=a+b) -->
<!--   X = -log(U) -->
<!--   X = apply(X[1:a,], 2, sum) / apply(X, 2, sum) -->
<!-- } -->
<!-- test2 = function(N, a, b) { -->
<!--   rbeta(N, a, b) -->
<!-- } -->
<!-- system.time(test1(N, a, b)); system.time(test2(N, a, b)) -->
<!-- ``` -->

<!-- ### (b), (c) -->
<!-- Omit. -->






<!-- ## Exercise 2.13 -->


<!-- ## Exercise 2.14 -->

<!-- ### (a) -->
<!-- Omit -->

<!-- ### (b) -->
<!-- ```{r} -->
<!-- N = 1000 -->
<!-- r = 10 -->

<!-- par(mfrow=c(1,3)) -->
<!-- for (p in c(0.01, 0.1, 0.5)) { -->
<!--   tmean = r*(1-p)/p -->
<!--   tstd = sqrt(r*(1-p)/p^2) -->
<!--   mrange = round(seq(max(0, tmean - 3*tstd), tmean + 3*tstd, 1)) -->
<!--   prob = pnbinom(mrange, r, p) -->
<!--   X = rep(0, N) -->

<!--   for (i in 1:N) { -->
<!--     u = runif(1) -->
<!--     X[i] = mrange[1] + sum(prob < u) -->
<!--   } -->

<!--   hist(X, freq=FALSE, main=paste("Negative Binomial with p=", p, sep="")) -->
<!--   lines(dnbinom(seq(1, max(X)), r, p), col="sienna", lwd=2) -->
<!-- } -->
<!-- ``` -->

<!-- ### (c) -->
<!-- We can calculate mean and variance of the logarithmic series distribution. -->

<!-- - $E[X] = \sum_{n=1}^\infty \frac{-(1-p)^n}{n \log p} \cdot n = \frac{-(1-p)}{p \log p}$. -->
<!-- - $E[X^2] = \sum_{n=1}^\infty \frac{-(1-p)^n}{n \log p} \cdot n = \frac{-(1-p)}{p^2 \log p}$. -->

<!-- Note that the minimum value of $X$ is $1$, not $0$. -->
<!-- ```{r} -->
<!-- N = 1000 -->

<!-- par(mfrow=c(1, 3)) -->
<!-- for (p in c(0.001, 0.01, 0.5)) { -->
<!--   tmean = -(1-p)/(p*log(p)) -->
<!--   tvar = -(1-p)/(p^2*log(p)) - tmean^2 -->
<!--   tstd = sqrt(tvar) -->
<!--   mrange = round(seq(max(1, tmean - 3*tstd), tmean + 3*tstd, 1)) -->
<!--   density = function(x, p) { -->
<!--     -(1-p)^x/(x*log(p)) -->
<!--   } -->
<!--   prob = cumsum(density(mrange, p)) -->
<!--   X = rep(1, N) -->

<!--   for (i in 1:N) { -->
<!--     u = runif(1) -->
<!--     X[i] = mrange[1] + sum(prob < u) -->
<!--   } -->

<!--   M = max(X) -->

<!--   hist(X, freq=FALSE, main=paste("Logarithmic Series with p=", p, sep="")) -->
<!--   lines(density(seq(1, M), p), col="sienna", lwd=2) -->
<!-- } -->
<!-- ``` -->

<!-- The histogram when $p=0.5$ seems to be an error in R. The first two histogram show that the tail distribution is so heavy, in which it is not unlikable to be happened even outside of three times standard deviation further from the mean. -->


<!-- ## Exercise 2.15 -->
<!-- ```{R} -->
<!-- N = 1000 -->

<!-- test1 = function(N, lambda) { -->
<!--   X = rep(0, N) -->
<!--   for (i in 1:N) { -->
<!--     K = 0 -->
<!--     tTime = 0 -->
<!--     while (TRUE) { -->
<!--       tTime = tTime + rexp(1, lambda) -->
<!--       if (tTime > 1) -->
<!--         break -->
<!--       K = K + 1 -->
<!--     } -->

<!--     X[i] = K -->
<!--   } -->
<!--   return(X) -->
<!-- } -->

<!-- test2 = function(N, lambda) { -->
<!--   X = rep(0, N) -->
<!--   for (i in 1:N) { -->
<!--     X[i] = rpois(1, lambda) -->
<!--   } -->
<!--   return(X) -->
<!-- } -->

<!-- test3 = function(N, lambda) { -->
<!--   X = rep(0, N) -->
<!--   spread = 3 * sqrt(lambda) -->
<!--   t = round(seq(max(0, lambda - spread), lambda + spread, 1)) -->
<!--   prob = ppois(t, lambda) -->
<!--   for (i in 1:N) { -->
<!--     u = runif(1) -->
<!--     X[i] = t[1] + sum(prob < u) -->
<!--   } -->
<!--   return(X) -->
<!-- } -->
<!-- ``` -->

<!-- 1. Small $\lambda(=5)$ -->
<!-- ```{R} -->
<!-- lambda = 5 -->

<!-- system.time(test1(N, lambda)) -->
<!-- system.time(test2(N, lambda)) -->
<!-- system.time(test3(N, lambda)) -->

<!-- par(mfrow=c(1, 3)) -->
<!-- X1 = test1(N, lambda) -->
<!-- X2 = test2(N, lambda) -->
<!-- X3 = test3(N, lambda) -->
<!-- Xrange = seq(min(X1, X2, X3), max(X1, X2, X3)) -->

<!-- hist(X1, freq=FALSE, main="Generated by Poisson Process") -->
<!-- lines(Xrange, dpois(Xrange, lambda), col="sienna", lwd=2) -->
<!-- hist(X2, freq=FALSE, main="Generated by rpois") -->
<!-- lines(Xrange, dpois(Xrange, lambda), col="sienna", lwd=2) -->
<!-- hist(X3, freq=FALSE, main="Generated by Example 2.5") -->
<!-- lines(Xrange, dpois(Xrange, lambda), col="sienna", lwd=2) -->
<!-- ``` -->

<!-- 2. Large $\lambda(=100)$ -->
<!-- ```{R} -->
<!-- lambda = 100 -->

<!-- system.time(test1(N, lambda)) -->
<!-- system.time(test2(N, lambda)) -->
<!-- system.time(test3(N, lambda)) -->

<!-- par(mfrow=c(1, 3)) -->
<!-- X1 = test1(N, lambda) -->
<!-- X2 = test2(N, lambda) -->
<!-- X3 = test3(N, lambda) -->
<!-- Xrange = seq(min(X1, X2, X3), max(X1, X2, X3)) -->

<!-- hist(X1, freq=FALSE, main="Generated by Poisson Process") -->
<!-- lines(Xrange, dpois(Xrange, lambda), col="sienna", lwd=2) -->
<!-- hist(X2, freq=FALSE, main="Generated by rpois") -->
<!-- lines(Xrange, dpois(Xrange, lambda), col="sienna", lwd=2) -->
<!-- hist(X3, freq=FALSE, main="Generated by Example 2.5") -->
<!-- lines(Xrange, dpois(Xrange, lambda), col="sienna", lwd=2) -->
<!-- ``` -->


<!-- ## Exercise 2.16 -->
<!-- ```{r} -->
<!-- N = 100000 -->

<!-- test1 = function(N, a, b) { -->
<!--   numor = runif(N)^(1/a) -->
<!--   denom = numor + runif(N)^(1/b) -->
<!--   numor = numor[denom <= 1] -->
<!--   denom = denom[denom <= 1] -->

<!--   X = numor / denom -->
<!--   return(X) -->
<!-- } -->

<!-- test2 = function(N, a, b) { -->
<!--   return(rbeta(N, a, b)) -->
<!-- } -->

<!-- test3 = function(N, a, b) { -->
<!--   U1 = rexp(a*N, 1) -->
<!--   U1 = matrix(data=U1, nrow=a) -->
<!--   U2 = rexp(b*N, 1) -->
<!--   U2 = matrix(data=U2, nrow=b) -->

<!--   numor = apply(U1, 2, sum) -->
<!--   denom = numor + apply(U2, 2, sum) -->
<!--   X = numor / denom -->

<!--   return(X) -->
<!-- } -->
<!-- ``` -->

<!-- 1. Small $a(=3)$ and $b(=6)$ -->

<!-- ```{r} -->
<!-- a = 3 -->
<!-- b = 8 -->

<!-- system.time(test1(N, a, b)) -->
<!-- system.time(test2(N, a, b)) -->
<!-- system.time(test3(N, a, b)) -->

<!-- Xrange = seq(0, 1, 0.001) -->
<!-- X1 = test1(N, a, b) -->
<!-- X2 = test2(N, a, b) -->
<!-- X3 = test3(N, a, b) -->

<!-- par(mfrow=c(1, 3)) -->
<!-- hist(X1, freq=FALSE, breaks=seq(0, 1, 0.05), main="Generated by uniform distribution") -->
<!-- lines(Xrange, dbeta(Xrange, a, b), col="sienna", lwd=2) -->
<!-- hist(X2, freq=FALSE, breaks=seq(0, 1, 0.05), main="Generated by rbeta") -->
<!-- lines(Xrange, dbeta(Xrange, a, b), col="sienna", lwd=2) -->
<!-- hist(X3, freq=FALSE, breaks=seq(0, 1, 0.05), main="Generated by exponential distribution") -->
<!-- lines(Xrange, dbeta(Xrange, a, b), col="sienna", lwd=2) -->
<!-- ``` -->

<!-- 2. Large $a$ and $b$ -->
<!-- Omit. -->


<!-- ## Exercise 2.17 -->


<!-- ## Exercise 2.18 -->
<!-- ```{r} -->
<!-- N = 2500 -->
<!-- supp = seq(-3, 3, 0.001) -->

<!-- h = function(x) { exp(-x^2/2)*(sin(6*x)^2 + 3*cos(x)^2*sin(4*x)^2 + 1) } -->
<!-- g = dnorm -->
<!-- M = 5 * sqrt(2 * pi) -->
<!-- X = rnorm(N); X = X[-3 < X & X < 3] -->
<!-- U = runif(length(X)) -->
<!-- Y = M * U * g(X) -->

<!-- AcceptX = X[Y <= h(X)] -->
<!-- RejectX = X[Y > h(X)] -->
<!-- rate = length(AcceptX) / N -->
<!-- c = rate * M -->
<!-- AcceptY = Y[Y <= h(X)] / c -->
<!-- RejectY = Y[Y > h(X)] / c -->
<!-- f = function(x) { h(x) / c } -->

<!-- plot(RejectX, RejectY, xlab="", ylab="", pch=20, cex=0.5, col="gray") -->
<!-- points(AcceptX, AcceptY, pch=20, cex=0.5) -->
<!-- lines(supp, g(supp) / rate, col="orange", lwd=2) -->
<!-- lines(supp, f(supp), col="red", lwd=2) -->
<!-- legend("topright", legend=c("g(x)", "f(x)"), col=c("orange", "red"), lty=c(1, 1)) -->
<!-- ``` -->

<!-- ## Exercise 2.19 -->
<!-- ```{r} -->
<!-- N = 2500 -->
<!-- supp = seq(-5, 5, 0.001) -->

<!-- f = dnorm -->
<!-- g = function(x) { exp(-abs(x))/2 } -->
<!-- M = sqrt(2 * exp(1) / pi) -->
<!-- X = rexp(N) * (2*rbinom(N, 1, 0.5) - 1); X = X[-5 < X & X < 5] -->
<!-- U = runif(length(X)) -->
<!-- Y = M * U * g(X) -->

<!-- AcceptX = X[Y <= f(X)] -->
<!-- RejectX = X[Y > f(X)] -->
<!-- AcceptY = Y[Y <= f(X)] -->
<!-- RejectY = Y[Y > f(X)] -->

<!-- plot(RejectX, RejectY, xlab="", ylab="", pch=20, cex=0.5, col="gray") -->
<!-- points(AcceptX, AcceptY, pch=20, cex=0.5) -->
<!-- lines(supp, g(supp) * M, col="orange", lwd=2) -->
<!-- lines(supp, f(supp), col="red", lwd=2) -->
<!-- legend("topright", legend=c("g(x)", "f(x)"), col=c("orange", "red"), lty=c(1, 1)) -->
<!-- ``` -->

<!-- ## Exercise 2.20 -->
<!-- ### (a) -->
<!-- ```{r} -->
<!-- N = 2500 -->
<!-- supp = seq(-3, 3, 0.001) -->

<!-- f = dnorm -->
<!-- g = dcauchy -->
<!-- M = sqrt(2*pi / exp(1)) -->
<!-- X = rcauchy(N); X = X[-3 < X & X < 3] -->
<!-- U = runif(length(X)) -->
<!-- Y = M * U * g(X) -->

<!-- AcceptX = X[Y <= f(X)] -->
<!-- RejectX = X[Y > f(X)] -->
<!-- AcceptY = Y[Y <= f(X)] -->
<!-- RejectY = Y[Y > f(X)] -->

<!-- plot(RejectX, RejectY, xlab="", ylab="", pch=20, cex=0.5, col="gray") -->
<!-- points(AcceptX, AcceptY, pch=20, cex=0.5) -->
<!-- lines(supp, g(supp) * M, col="orange", lwd=2) -->
<!-- lines(supp, f(supp), col="red", lwd=2) -->
<!-- legend("topright", legend=c("g(x)", "f(x)"), col=c("orange", "red"), lty=c(1, 1)) -->
<!-- ``` -->

<!-- ### (b) -->
<!-- ```{r} -->
<!-- N = 2500 -->
<!-- supp = seq(0, 3, 0.001) -->

<!-- f = function(x) { dgamma(x, 4.3, 6.2) } -->
<!-- g = function(x) { dgamma(x, 4, 6) } -->
<!-- f_g = function(x) { f(x)/g(x) } -->
<!-- M = optimize(f_g, c(0, 100), maximum=TRUE)$objective -->
<!-- X = rgamma(N, 4, 6) -->
<!-- U = runif(N) -->
<!-- Y = M * U * g(X) -->

<!-- AcceptX = X[Y <= f(X)] -->
<!-- RejectX = X[Y > f(X)] -->
<!-- AcceptY = Y[Y <= f(X)] -->
<!-- RejectY = Y[Y > f(X)] -->

<!-- plot(RejectX, RejectY, xlim=c(0, 3), xlab="", ylab="", pch=20, cex=0.5, col="gray") -->
<!-- points(AcceptX, AcceptY, pch=20, cex=0.5) -->
<!-- lines(supp, g(supp) * M, col="orange", lwd=2) -->
<!-- lines(supp, f(supp), col="red", lwd=2) -->
<!-- legend("topright", legend=c("g(x)", "f(x)"), col=c("orange", "red"), lty=c(1, 1)) -->
<!-- ``` -->


<!-- ## Exercise 2.21 -->
<!-- ## Exercise 2.22 -->
<!-- ### (a) -->
<!-- ```{R} -->
<!-- N = 10^4 -->
<!-- mu = 0 -->
<!-- sigma = 1 -->

<!-- test1 = function(N, mu, sigma, a) { -->
<!--   X = rep(0, N) -->
<!--   for (i in 1:N) { -->
<!--     z = a - 1 -->
<!--     while (z < a) z = rnorm(1, mu, sigma) -->
<!--     X[i] = z -->
<!--   } -->
<!--   return(X) -->
<!-- } -->

<!-- system.time(test1(N, mu, sigma, 0)) -->
<!-- system.time(test1(N, mu, sigma, 1)) -->
<!-- system.time(test1(N, mu, sigma, 2)) -->

<!-- X1 = test1(N, mu, sigma, 0) -->
<!-- X2 = test1(N, mu, sigma, 1) -->
<!-- X3 = test1(N, mu, sigma, 2) -->
<!-- xmax = max(X1, X2, X3) -->

<!-- par(mfrow=c(1, 3)) -->
<!-- hist(X1, freq=FALSE, main="Truncated at a=1") -->
<!-- hist(X2, freq=FALSE, main="Truncated at a=2") -->
<!-- hist(X3, freq=FALSE, main="Truncated at a=3") -->
<!-- ``` -->

<!-- ### (b) -->
<!-- One can check that the algorithm in the exercise is the Accept-Reject methods for  $f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{(x-\mu)^2}{2\sigma^2}) 1_{[a, \infty)}(x) / \Phi(-\frac{a-\mu}{\sigma})$, $g(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{(x-\mu)^2}{2\sigma^2})$, and  $M = 1/\Phi(-\frac{a-\mu}{\sigma})$; -->
<!-- For $Y \sim N(\mu, \sigma^2)$ we have $\frac{1}{M} \frac{f(Y)}{g(Y)} = 1_{[a, \infty)}(Y)$, hence for any $U \sim U(0, 1)$, we will accept whenever $Y \geq a$. The code in the exercise just implemented this. -->

<!-- The acceptance rate is $1/M$, which will be significantly small when $a$ is large enough, i.e. $a$ is in the tail distribution. -->

### (c)
Let $f(x) = \frac{1}{\sqrt{2 \pi}} \exp(-\frac{x^2}{2}) 1_{[a, \infty)}(x) / \Phi(-a)$ and $g(x) = \frac{1}{\sqrt{2 \pi}} \exp(-\frac{(x - \bar\mu)^2}{2})$. 
Then one can check that $f(x)/g(x) = \exp(-x\bar\mu + \frac{\bar\mu^2}{2}) 1_{[a, \infty)}(x) / \Phi(-a)$ is maximized when $x=a$.
Therefore we define the constant $M = \exp(-\bar\mu a + \frac{\bar\mu^2}{2}) / \Phi(-a)$ so that we have $f(x) \leq Mg(x)$.
On the other hand, we must minimize this $M = \exp(\frac{(\bar\mu - a)^2}{2} - \frac{a^2}{2})$ w.r.t $\bar\mu$.
Since $\bar\mu > 0$, there is no optimal $\bar\mu$ if $a \leq 0$. If $a > 0$ then $a$ is the optimal $\bar\mu$.

### (d)
Let $f(x) = \frac{1}{\sqrt{2 \pi}} \exp(-\frac{x^2}{2}) 1_{[a, \infty)}(x) / \Phi(-a)$ and $g_\alpha (x) = \alpha \exp(-\alpha(z-a)) 1_{[a, \infty)}(x)$.
In the same way, one can check that $f(x)/g_\alpha(x) = \frac{1}{\alpha \sqrt{2 \pi}} \exp(-\frac{x^2}{2} + \alpha x - a\alpha) / \Phi(-a) 1_{[a, \infty)}(x) \propto \exp(\alpha(x-a)) \exp(-x^2/2)$ is maximized when $x = \max(a, \alpha)$.
Since $\alpha \geq a$, the ratio attains maximum $\frac{1}{\alpha \sqrt{2 \pi}} \exp(\frac{\alpha^2}{2} - a\alpha) / \Phi(-a)$ when $x = \alpha$.
Moreover, $\alpha = a$ still makes a legitimate candidate density.


## Exercise 2.23